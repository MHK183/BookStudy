{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4c0851",
   "metadata": {},
   "source": [
    "## 오차제곱합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ba89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbdae3",
   "metadata": {},
   "source": [
    "## 교차엔트로피 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d74263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # log 안의 값이 0이 되지 않도록 매우 작은 값을 더해줌\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af2605",
   "metadata": {},
   "source": [
    "## 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437bf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445618dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1ebaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0] # 60000\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ff9bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31629, 16416, 57962, 44650, 48822,  1268, 36231, 16035, 14114,\n",
       "       21754])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcf54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t): # y 신경망의 출력, t 정답 레이블\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a17a81",
   "metadata": {},
   "source": [
    "## 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4861bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / 2*h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30cdde",
   "metadata": {},
   "source": [
    "## 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d72630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6449aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size): \n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef7518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f5416",
   "metadata": {},
   "source": [
    "## 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b225e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        print(x)\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        print(grad)\n",
    "       \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10ece2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  4.]\n",
      "[-6.  8.]\n",
      "[-2.4  3.2]\n",
      "[-4.8  6.4]\n",
      "[-1.92  2.56]\n",
      "[-3.84  5.12]\n",
      "[-1.536  2.048]\n",
      "[-3.072  4.096]\n",
      "[-1.2288  1.6384]\n",
      "[-2.4576  3.2768]\n",
      "[-0.98304  1.31072]\n",
      "[-1.96608  2.62144]\n",
      "[-0.786432  1.048576]\n",
      "[-1.572864  2.097152]\n",
      "[-0.6291456  0.8388608]\n",
      "[-1.2582912  1.6777216]\n",
      "[-0.50331648  0.67108864]\n",
      "[-1.00663296  1.34217728]\n",
      "[-0.40265318  0.53687091]\n",
      "[-0.80530637  1.07374182]\n",
      "[-0.32212255  0.42949673]\n",
      "[-0.64424509  0.85899346]\n",
      "[-0.25769804  0.34359738]\n",
      "[-0.51539608  0.68719477]\n",
      "[-0.20615843  0.27487791]\n",
      "[-0.41231686  0.54975581]\n",
      "[-0.16492674  0.21990233]\n",
      "[-0.32985349  0.43980465]\n",
      "[-0.1319414   0.17592186]\n",
      "[-0.26388279  0.35184372]\n",
      "[-0.10555312  0.14073749]\n",
      "[-0.21110623  0.28147498]\n",
      "[-0.08444249  0.11258999]\n",
      "[-0.16888499  0.22517998]\n",
      "[-0.06755399  0.09007199]\n",
      "[-0.13510799  0.18014399]\n",
      "[-0.0540432   0.07205759]\n",
      "[-0.10808639  0.14411519]\n",
      "[-0.04323456  0.05764608]\n",
      "[-0.08646911  0.11529215]\n",
      "[-0.03458765  0.04611686]\n",
      "[-0.06917529  0.09223372]\n",
      "[-0.02767012  0.03689349]\n",
      "[-0.05534023  0.07378698]\n",
      "[-0.02213609  0.02951479]\n",
      "[-0.04427219  0.05902958]\n",
      "[-0.01770887  0.02361183]\n",
      "[-0.03541775  0.04722366]\n",
      "[-0.0141671   0.01888947]\n",
      "[-0.0283342   0.03777893]\n",
      "[-0.01133368  0.01511157]\n",
      "[-0.02266736  0.03022315]\n",
      "[-0.00906694  0.01208926]\n",
      "[-0.01813389  0.02417852]\n",
      "[-0.00725355  0.00967141]\n",
      "[-0.01450711  0.01934281]\n",
      "[-0.00580284  0.00773713]\n",
      "[-0.01160569  0.01547425]\n",
      "[-0.00464228  0.0061897 ]\n",
      "[-0.00928455  0.0123794 ]\n",
      "[-0.00371382  0.00495176]\n",
      "[-0.00742764  0.00990352]\n",
      "[-0.00297106  0.00396141]\n",
      "[-0.00594211  0.00792282]\n",
      "[-0.00237684  0.00316913]\n",
      "[-0.00475369  0.00633825]\n",
      "[-0.00190148  0.0025353 ]\n",
      "[-0.00380295  0.0050706 ]\n",
      "[-0.00152118  0.00202824]\n",
      "[-0.00304236  0.00405648]\n",
      "[-0.00121694  0.00162259]\n",
      "[-0.00243389  0.00324519]\n",
      "[-0.00097356  0.00129807]\n",
      "[-0.00194711  0.00259615]\n",
      "[-0.00077884  0.00103846]\n",
      "[-0.00155769  0.00207692]\n",
      "[-0.00062308  0.00083077]\n",
      "[-0.00124615  0.00166153]\n",
      "[-0.00049846  0.00066461]\n",
      "[-0.00099692  0.00132923]\n",
      "[-0.00039877  0.00053169]\n",
      "[-0.00079754  0.00106338]\n",
      "[-0.00031901  0.00042535]\n",
      "[-0.00063803  0.00085071]\n",
      "[-0.00025521  0.00034028]\n",
      "[-0.00051042  0.00068056]\n",
      "[-0.00020417  0.00027223]\n",
      "[-0.00040834  0.00054445]\n",
      "[-0.00016334  0.00021778]\n",
      "[-0.00032667  0.00043556]\n",
      "[-0.00013067  0.00017422]\n",
      "[-0.00026134  0.00034845]\n",
      "[-0.00010453  0.00013938]\n",
      "[-0.00020907  0.00027876]\n",
      "[-8.36277945e-05  1.11503726e-04]\n",
      "[-0.00016726  0.00022301]\n",
      "[-6.69022356e-05  8.92029808e-05]\n",
      "[-0.0001338   0.00017841]\n",
      "[-5.35217885e-05  7.13623846e-05]\n",
      "[-0.00010704  0.00014272]\n",
      "[-4.28174308e-05  5.70899077e-05]\n",
      "[-8.56348616e-05  1.14179815e-04]\n",
      "[-3.42539446e-05  4.56719262e-05]\n",
      "[-6.85078892e-05  9.13438523e-05]\n",
      "[-2.74031557e-05  3.65375409e-05]\n",
      "[-5.48063114e-05  7.30750819e-05]\n",
      "[-2.19225246e-05  2.92300327e-05]\n",
      "[-4.38450491e-05  5.84600655e-05]\n",
      "[-1.75380196e-05  2.33840262e-05]\n",
      "[-3.50760393e-05  4.67680524e-05]\n",
      "[-1.40304157e-05  1.87072210e-05]\n",
      "[-2.80608314e-05  3.74144419e-05]\n",
      "[-1.12243326e-05  1.49657768e-05]\n",
      "[-2.24486651e-05  2.99315535e-05]\n",
      "[-8.97946606e-06  1.19726214e-05]\n",
      "[-1.79589321e-05  2.39452428e-05]\n",
      "[-7.18357285e-06  9.57809713e-06]\n",
      "[-1.43671457e-05  1.91561943e-05]\n",
      "[-5.74685828e-06  7.66247770e-06]\n",
      "[-1.14937166e-05  1.53249554e-05]\n",
      "[-4.59748662e-06  6.12998216e-06]\n",
      "[-9.19497325e-06  1.22599643e-05]\n",
      "[-3.67798930e-06  4.90398573e-06]\n",
      "[-7.35597860e-06  9.80797146e-06]\n",
      "[-2.94239144e-06  3.92318858e-06]\n",
      "[-5.88478288e-06  7.84637717e-06]\n",
      "[-2.35391315e-06  3.13855087e-06]\n",
      "[-4.70782630e-06  6.27710174e-06]\n",
      "[-1.88313052e-06  2.51084069e-06]\n",
      "[-3.76626104e-06  5.02168139e-06]\n",
      "[-1.50650442e-06  2.00867256e-06]\n",
      "[-3.01300883e-06  4.01734511e-06]\n",
      "[-1.20520353e-06  1.60693804e-06]\n",
      "[-2.41040707e-06  3.21387609e-06]\n",
      "[-9.64162827e-07  1.28555044e-06]\n",
      "[-1.92832565e-06  2.57110087e-06]\n",
      "[-7.71330261e-07  1.02844035e-06]\n",
      "[-1.54266052e-06  2.05688070e-06]\n",
      "[-6.17064209e-07  8.22752279e-07]\n",
      "[-1.23412842e-06  1.64550456e-06]\n",
      "[-4.93651367e-07  6.58201823e-07]\n",
      "[-9.87302734e-07  1.31640365e-06]\n",
      "[-3.94921094e-07  5.26561458e-07]\n",
      "[-7.89842188e-07  1.05312292e-06]\n",
      "[-3.15936875e-07  4.21249167e-07]\n",
      "[-6.31873750e-07  8.42498333e-07]\n",
      "[-2.52749500e-07  3.36999333e-07]\n",
      "[-5.05499000e-07  6.73998667e-07]\n",
      "[-2.02199600e-07  2.69599467e-07]\n",
      "[-4.04399200e-07  5.39198933e-07]\n",
      "[-1.61759680e-07  2.15679573e-07]\n",
      "[-3.23519360e-07  4.31359147e-07]\n",
      "[-1.29407744e-07  1.72543659e-07]\n",
      "[-2.58815488e-07  3.45087317e-07]\n",
      "[-1.03526195e-07  1.38034927e-07]\n",
      "[-2.07052390e-07  2.76069854e-07]\n",
      "[-8.28209562e-08  1.10427942e-07]\n",
      "[-1.65641912e-07  2.20855883e-07]\n",
      "[-6.62567649e-08  8.83423532e-08]\n",
      "[-1.32513530e-07  1.76684706e-07]\n",
      "[-5.30054119e-08  7.06738826e-08]\n",
      "[-1.06010824e-07  1.41347765e-07]\n",
      "[-4.24043296e-08  5.65391061e-08]\n",
      "[-8.48086591e-08  1.13078212e-07]\n",
      "[-3.39234636e-08  4.52312849e-08]\n",
      "[-6.78469273e-08  9.04625697e-08]\n",
      "[-2.71387709e-08  3.61850279e-08]\n",
      "[-5.42775418e-08  7.23700558e-08]\n",
      "[-2.17110167e-08  2.89480223e-08]\n",
      "[-4.34220335e-08  5.78960446e-08]\n",
      "[-1.73688134e-08  2.31584178e-08]\n",
      "[-3.47376268e-08  4.63168357e-08]\n",
      "[-1.38950507e-08  1.85267343e-08]\n",
      "[-2.77901014e-08  3.70534686e-08]\n",
      "[-1.11160406e-08  1.48213874e-08]\n",
      "[-2.22320811e-08  2.96427748e-08]\n",
      "[-8.89283245e-09  1.18571099e-08]\n",
      "[-1.77856649e-08  2.37142199e-08]\n",
      "[-7.11426596e-09  9.48568795e-09]\n",
      "[-1.42285319e-08  1.89713759e-08]\n",
      "[-5.69141277e-09  7.58855036e-09]\n",
      "[-1.13828255e-08  1.51771007e-08]\n",
      "[-4.55313022e-09  6.07084029e-09]\n",
      "[-9.10626043e-09  1.21416806e-08]\n",
      "[-3.64250417e-09  4.85667223e-09]\n",
      "[-7.28500835e-09  9.71334446e-09]\n",
      "[-2.91400334e-09  3.88533778e-09]\n",
      "[-5.82800668e-09  7.77067557e-09]\n",
      "[-2.33120267e-09  3.10827023e-09]\n",
      "[-4.66240534e-09  6.21654046e-09]\n",
      "[-1.86496214e-09  2.48661618e-09]\n",
      "[-3.72992427e-09  4.97323236e-09]\n",
      "[-1.49196971e-09  1.98929295e-09]\n",
      "[-2.98393942e-09  3.97858589e-09]\n",
      "[-1.19357577e-09  1.59143436e-09]\n",
      "[-2.38715153e-09  3.18286871e-09]\n",
      "[-9.54860614e-10  1.27314749e-09]\n",
      "[-1.90972123e-09  2.54629497e-09]\n",
      "[-7.63888491e-10  1.01851799e-09]\n",
      "[-1.52777698e-09  2.03703598e-09]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사법으로 함수 f의 최솟값을 구하라\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef18632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c62d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) # 정규분포로 초기화\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b4a4f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81545957  0.60626962 -1.76149833]\n",
      " [ 1.0127327   0.99372941  2.51080156]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e303579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42218369 1.25811824 1.20282241]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ede4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) # 최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f45531a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222592452197866"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0be9f",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "650c1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f741fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size,  hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f09a1f",
   "metadata": {},
   "source": [
    "## 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71be451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "2.2913691245532313\n",
      "2.290514724048532\n",
      "2.27531511929577\n",
      "2.2905102113963896\n",
      "2.3071452310448497\n",
      "2.300860875030318\n",
      "2.2894065338997422\n",
      "2.290141978421042\n",
      "2.2986255283757844\n",
      "2.289106351646101\n",
      "2.287014362830554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[0;32m     19\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m t_train[batch_mask]\n\u001b[1;32m---> 21\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     24\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad[key]\n",
      "Cell \u001b[1;32mIn[21], line 37\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     34\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     36\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 37\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Desktop\\KMH\\study\\AI\\common\\gradient.py:46\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[1;32m---> 46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m grad[idx] \u001b[38;5;241m=\u001b[39m (fxh1 \u001b[38;5;241m-\u001b[39m fxh2) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mh)\n\u001b[0;32m     49\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;66;03m# 값 복원\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 34\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     37\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 21\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m z1 \u001b[38;5;241m=\u001b[39m sigmoid(a1)\n\u001b[0;32m     15\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m---> 16\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\Desktop\\KMH\\study\\AI\\common\\functions.py:35\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mexp(x), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mT \n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x) \u001b[38;5;66;03m# 오버플로 대책\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
